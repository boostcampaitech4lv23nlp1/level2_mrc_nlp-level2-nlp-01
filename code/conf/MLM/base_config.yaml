train:
    model:
        model_name: "klue/bert-base"
        num_train_epochs: 5
        learning_rate: 5e-05
        per_device_train_batch_size: 64
        per_device_eval_batch_size: 64
        eval_steps: 50
        save_steps: 100
        logging_steps: 50
        warmup_steps: 50
        weight_decay: 1e-02
        save_total_limit: 2
        max_seq_length: 40
        masking_probability: 0.15
        do_whole_word_mask: False
    dir:
        data_dir: "/opt/ml/input/data/train_dataset"
        output_dir: "/opt/ml/models/pretrained_mlm"
    stage:
        do_eval: True

    